{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:02.634211Z",
     "start_time": "2025-06-18T19:16:45.946165Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Wetter- und Luftqualitätsdaten\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m----> 3\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWetter- und Luftqualitätsdaten\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py:228\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    231\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py:392\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 392\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py:144\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 144\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m    147\u001B[0m                   conf, jsc, profiler_cls)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py:339\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[1;32m--> 339\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    340\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\java_gateway.py:105\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m proc\u001B[38;5;241m.\u001B[39mpoll() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[1;32m--> 105\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Woher kommen die Daten und in welcher Form liegen sie vor?\n",
    "\n",
    "#### Wetterdaten Zürich und St. Gallen\n",
    "Die Wetterdaten von Zürich und St. Gallen stammen vom Bundesamt für Meteorologie und Kimatologie - MeteoSchweiz. Abrufbar unter folgendem Link: xxx\n",
    "\n",
    "Es handelt sich hierbei um tägliche Messwerte, die als CSV-Dateien im UFT-8 Format vorliegen. Das Datumsfeld trägt den Namen *reference_timestamp* und ist im Format DD.MM.YYYY HH:MM als Zeichenkette gespeichert. Die Dateien umfassen Messwerte aus zwei Wetterstationen: \n",
    "\n",
    "* Zürich Fluntern (SMA)\n",
    "* St. Gallen (STG)\n",
    "\n",
    "#### Luftqualitätsdaten Zürich und St. Gallen\n",
    "\n",
    "Die Luftqualitätsdaten stammen von der Plattform OSTLUFT abrufbar unter folgendem Link: https://www.ostluft.ch/messwerte/datenabfrage, welche von versch. kantonalen Fachstellen getragen wird, darunter das Amt für Abfall, Wasser, Energie und Luft (AWEL) des Kantons Zürich sowie das Amt für Umwelt des Kantons St. Gallen. \n",
    "\n",
    "Die Daten wurden über die offizielle Datenabfrage-Schnittstelle heruntergeladen und liegen als CSV-Dateien im UFT-8 Format vor. Es handelt sich um aggregierte Tagesmittelwerte wichtiger Parameter für Luftschadstoffe.\n",
    "\n",
    "Das Datumsfeld heisst *Startzeit* udn ist im Format TT.MM.YYYY HH:MM vor. Die Messwerte stammen von zwei festen Luftmessstationen:\n",
    "\n",
    "* **Zürich**: Station Zch_Stampfenbachstr\n",
    "* **St. Gallen**: Station StG_St.Leonhard-Str"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Schritt 1: Wetterdaten einlesen\n",
    "\n",
    "# Zürich\n",
    "df_zurich = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(\"data/weather/zurich/ogd-smn_sma_d_historical.csv\")\n",
    "\n",
    "# St. Gallen\n",
    "df_stgallen = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(\"data/weather/stgallen/ogd-smn_stg_d_historical.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Zürich\n",
    "df_air_zurich = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"data/air_quality/airquality_zurich.csv\") \\\n",
    "    .withColumn(\"location\", lit(\"Zürich\"))\n",
    "\n",
    "# St. Gallen\n",
    "df_air_stgallen = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"data/air_quality/airquality_stgallen.csv\") \\\n",
    "    .withColumn(\"location\", lit(\"St. Gallen\"))\n",
    "\n",
    "# Zusammenführen\n",
    "df_air = df_air_zurich.unionByName(df_air_stgallen)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Pivotierung der Luftqualitätsdaten\n",
    "\n",
    "Das ursprüngliche Format der Luftqualitätsdaten lag im \"Long Format\" vor. In dieser Struktur entsprach jede Zeile einen einzelnen Messswert, der durch Datum, Standort sowie Schadstoffparameter bestimmt war. Um die Daten jedoch effizient analysieren und mit den Wetterdaten veknüpfen zu könenn, wird eine Pivotierung vorgenommen. Das Ergebnis ist ein \"Wide Format\", bei dem jeder schadstoff als eigene Spalte dargestellt ist. So enthält jede Zeile alle relevanten Messwerte eines Tages für einen bestimmten Standort."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import to_date, col, first, when\n",
    "\n",
    "# Bereinigung\n",
    "df_air_clean = df_air\n",
    "\n",
    "# Pivotierung\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "df_air_clean = df_air_clean.withColumn(\n",
    "    \"date\", to_date(col(\"Startzeit\"), \"dd.MM.yyyy HH:mm\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, first\n",
    "\n",
    "df_luft_pivot = df_air_clean.groupBy(\"date\", \"location\").agg(\n",
    "    first(col(\"PM10\")).alias(\"PM10\"),\n",
    "    first(col(\"`PM2.5`\")).alias(\"PM2.5\"),\n",
    "    first(col(\"Ozon\")).alias(\"Ozon\"),\n",
    "    first(col(\"CO\")).alias(\"CO\"),\n",
    "    first(col(\"NO2\")).alias(\"NO2\"),\n",
    "    first(col(\"NO\")).alias(\"NO\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Datumsformat vereinheitlichen\n",
    "\n",
    "Um die Verarbeitung sowie Verknüpfung der Wetter- und Luftqualitätsdaten zu ermöglichen, wird das Datumsformat in beiden Datsätzen vereinheitlicht. Die ursprüngliche Datums- und Zeitangabe liegen in unterschiedlcihen Formate vor und zwar:\n",
    "\n",
    "* **Wetterdaten**: DD.MM.YYYY HH:mm (reference_timestamp)\n",
    "* **Luftqualitätsdaten**: DD.MM.YYYY (Startzeit)\n",
    "\n",
    "Durch die Konvertierung in ein einheitliches Datums- und Zeitangabeformat wird sichergestellt, dass beide Quellen korrekt über das Datum gejoint werden können."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Wetterdaten: Datum umwandeln\n",
    "df_zurich = df_zurich.withColumn(\n",
    "    \"date\", to_date(col(\"reference_timestamp\"), \"dd.MM.yyyy HH:mm\")\n",
    ")\n",
    "df_stgallen = df_stgallen.withColumn(\n",
    "    \"date\", to_date(col(\"reference_timestamp\"), \"dd.MM.yyyy HH:mm\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_weather = df_zurich.unionByName(df_stgallen)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Standortbezeichnung vereinheitlichen\n",
    "\n",
    "Damit Wetter- und Luftqualitätsdaten korrekt zusammengeführt werden können, ist es notwendig, dass die Bezeichnung der Standorte gleich sind. Aktuell haben die Rohdaten noch unterschiedliche Standortbezeichnungen:\n",
    "\n",
    "* **Standort in Wetterdaten**: In der Spalte *station_abbr* finden sich SMA für Zürich und STG für St. Gallen\n",
    "* **Standort in Luftqualitätsdaten**: In der Spalte *location* finden wir Zch_Stampfenbachstr für Zürich und StG_St.Leonhard-Str für St. Gallen\n",
    "\n",
    "Damit der Join erfolgreich passieren wird, wird eine neue Spalte *location* erstellt, in der die Standorte konsistent mit **\"Zürich\"** und **\"St. Gallen\"** benannt werden können."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Standort Wetterdaten standardisieren\n",
    "if \"station_abbr\" in df_weather.columns:\n",
    "    df_weather_std = df_weather.withColumn(\n",
    "        \"location\",\n",
    "        when(col(\"station_abbr\") == \"SMA\", \"Zürich\")\n",
    "        .when(col(\"station_abbr\") == \"STG\", \"St. Gallen\")\n",
    "        .otherwise(\"Andere\")\n",
    "    )\n",
    "else:\n",
    "    print(\"Spalte 'station_abbr' nicht gefunden in df_zurich!\")\n",
    "    df_weather_std = df_zurich\n",
    "\n",
    "# Standort Luftqualitätsdaten standardisieren\n",
    "df_air_std = df_luft_pivot.withColumn(\n",
    "    \"location\",\n",
    "    when(col(\"location\").startswith(\"Zürich\"), \"Zürich\")\n",
    "    .when(col(\"location\").contains(\"St. Gallen\"), \"St. Gallen\")\n",
    "    .otherwise(\"Andere\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Join Wetter- und Luftqualitätsdaten\n",
    "\n",
    "Die Zusammenführung der Wetter- und Luftqualitätsdaten brauchen wir, damit wir später mit einem einheitlichen Datensatz weiter arbeiten können, der alle relevanten Merkmale für eine Modellierung beinhaltet. Die Wetter- und Luftqualitätsdaten sind grundsätzlich örtlich und zeitlich aufeinander abgestimtm worden und können nun miteinander verknüpft werden.\n",
    "\n",
    "Ziel ist es mögliche Wettereinflüsse auf die Luftqualität zu analysieren und so ein prädiktives Modell zu trainieren. Durch den Join stehen uns pro Zeile sowohl meterologische als auch umweltrelevante Parameter pro Tag und Standort zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_air_std = df_air_std.filter((col(\"date\").isNotNull()) & (year(\"date\") > 1900))\n",
    "df_weather_std = df_weather_std.filter((col(\"date\").isNotNull()) & (year(\"date\") > 1900))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_joined = df_air_std.join(\n",
    "    df_weather_std,\n",
    "    on=[\"date\", \"location\"],\n",
    "    how=\"inner\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the mapping\n",
    "mapping_df = spark.read.csv(\"data/weather/zurich/ogd-smn_meta_parameters.csv\", header=True, sep=\";\")\n",
    "\n",
    "# Convert to dictionary\n",
    "mapping_dict = {row['parameter_shortname']: row['parameter_description_en'] for row in mapping_df.collect()}\n",
    "\n",
    "# Rename columns\n",
    "for old_name, new_name in mapping_dict.items():\n",
    "    if old_name in df_joined.columns:\n",
    "        df_joined = df_joined.withColumnRenamed(old_name, new_name)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_joined = df_joined.drop('station_abbr','reference_timestamp','Longwave outgoing radiation daily mean','Shortwave reflected radiation daily mean','Diffuse radiation daily mean',)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_save = df_joined.toPandas()\n",
    "\n",
    "df_save.to_csv(\"data/cleaned/cleaned_Data.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
